{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\HENRY\\CURSO DE YOUTUBE\\AUTOMATI\\scraping\\ml\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Imprime el directorio de trabajo actual\n",
    "print(os.getcwd())\n",
    "\n",
    "# URL inicial para hacer scraping\n",
    "siguiente = 'https://listado.mercadolibre.com.ar/reloj-smartwatchh'\n",
    "\n",
    "# Listas para almacenar los datos extraídos\n",
    "lista_titulos = []\n",
    "lista_url = []\n",
    "lista_precios = []\n",
    "\n",
    "# Bucle para iterar sobre todas las páginas de resultados\n",
    "while True:\n",
    "    # Realiza una solicitud HTTP GET a la URL\n",
    "    r = requests.get(siguiente)\n",
    "    # Verifica si la solicitud fue exitosa\n",
    "    if r.status_code == 200:\n",
    "        # Crea un objeto BeautifulSoup a partir del contenido de la respuesta\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        # Crea un objeto DOM a partir del contenido de la respuesta\n",
    "        dom = etree.HTML(str(soup))\n",
    "        # Busca todos los elementos h2 con la clase especificada y extrae el texto de cada uno\n",
    "        titulos = soup.find_all('h2', attrs={\"class\": \"ui-search-item__title shops__item-title\"})\n",
    "        # Reemplaza todas las comas por espacios en blanco en cada título\n",
    "        titulos = [i.text.replace(',', ' ') for i in titulos]\n",
    "        # Agrega los títulos a la lista de títulos\n",
    "        lista_titulos.extend(titulos)\n",
    "        # Busca todos los elementos a con la clase especificada y extrae el valor del atributo href de cada uno\n",
    "        urls = soup.find_all('a', attrs={\"class\": \"ui-search-item__group__element shops__items-group-details ui-search-link\"})\n",
    "        urls = [i.get('href') for i in urls]\n",
    "        # Agrega las URLs a la lista de URLs\n",
    "        lista_url.extend(urls)\n",
    "        # Busca todos los elementos div con la clase especificada y extrae el texto de cada uno\n",
    "        precio = soup.find_all('div', attrs={\"class\": \"ui-search-price__second-line shops__price-second-line\"})\n",
    "        # Usa una expresión regular para extraer solo los dígitos y el punto decimal de cada texto de precio\n",
    "        precio = [re.search(r'\\d+\\.\\d+', i.text).group() if re.search(r'\\d+\\.\\d+', i.text) else '' for i in precio]\n",
    "        # Agrega los precios a la lista de precios\n",
    "        lista_precios.extend(precio)\n",
    "\n",
    "        # Busca el elemento span con la clase especificada y extrae el texto\n",
    "        botoninicial = soup.find('span', attrs={\"class\", \"andes-pagination__link\"}).text\n",
    "        # Convierte el texto a un número entero\n",
    "        botoninicial = int(botoninicial)\n",
    "        # Busca el elemento li con la clase especificada y extrae el texto\n",
    "        can = soup.find('li', attrs={\"class\", \"andes-pagination__page-count\"})\n",
    "        # Divide el texto en palabras y toma la segunda palabra (el número total de páginas)\n",
    "        can = int(can.text.split(\" \")[1])\n",
    "    else:\n",
    "        # Si la solicitud no fue exitosa, termina el bucle\n",
    "        break\n",
    "\n",
    "    # Si se ha llegado a la última página, termina el bucle\n",
    "    if botoninicial == can:\n",
    "        break\n",
    "\n",
    "    # Usa XPath para encontrar el elemento a que representa el botón \"Siguiente\" y extrae el valor del atributo href\n",
    "    siguiente = dom.xpath('//div[@class=\"ui-search-pagination shops__pagination-content\"]//ul/li[contains(@class,\"--next\")]/a')[0].get('href')\n",
    "\n",
    "# Abre un archivo CSV en modo escritura y codificación UTF-8\n",
    "with open('datos_ventas.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file, quoting=csv.QUOTE_NONNUMERIC)  # Agrega el parámetro quoting\n",
    "    # Escribe la fila de encabezados en el archivo CSV\n",
    "    writer.writerow(['Titulo', 'Precio', 'URL'])\n",
    "\n",
    "    # Itera sobre las listas de títulos, precios y URLs y escribe cada fila en el archivo CSV\n",
    "    for titulo, precio, url in zip(lista_titulos, lista_precios, lista_url):\n",
    "        writer.writerow([titulo, precio, url])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
